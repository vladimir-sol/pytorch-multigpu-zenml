"""Custom materializers for Hugging Face datasets and tokenizers.

This file has been generated by AI.
"""


from typing import Any, Type

import datasets
import transformers
from zenml.materializers.base_materializer import BaseMaterializer


class DatasetMaterializer(BaseMaterializer):
    """Materializer for Hugging Face datasets."""

    ASSOCIATED_TYPES = (datasets.Dataset,)
    ASSOCIATED_ARTIFACT_TYPES = (datasets.Dataset,)

    def load(self, data_type: Type[Any]) -> datasets.Dataset:
        """Load a dataset from disk.

        Args:
            data_type: The type of the dataset to load.

        Returns:
            The loaded dataset.
        """
        return datasets.load_from_disk(self.uri)

    def save(self, data: datasets.Dataset) -> None:
        """Save a dataset to disk.

        Args:
            data: The dataset to save.
        """
        data.save_to_disk(self.uri)


class TokenizerMaterializer(BaseMaterializer):
    """Materializer for Hugging Face tokenizers."""

    ASSOCIATED_TYPES = (transformers.PreTrainedTokenizerBase,)
    ASSOCIATED_ARTIFACT_TYPES = (transformers.PreTrainedTokenizerBase,)

    def load(self, data_type: Type[Any]) -> transformers.PreTrainedTokenizerBase:
        """Load a tokenizer from disk.

        Args:
            data_type: The type of the tokenizer to load.

        Returns:
            The loaded tokenizer.
        """
        return transformers.AutoTokenizer.from_pretrained(self.uri)

    def save(self, data: transformers.PreTrainedTokenizerBase) -> None:
        """Save a tokenizer to disk.

        Args:
            data: The tokenizer to save.
        """
        data.save_pretrained(self.uri)
